{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EEeen44JzUPl",
        "_VX27if0zXnf"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNq99GziWKW8stD6vPYh+2z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreyong15/Counsel.NLP/blob/main/Model/Fine_Tune_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install library"
      ],
      "metadata": {
        "id": "EEeen44JzUPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oooVp1muy0fV",
        "outputId": "b5de9f8f-9583-411b-fb59-dfe0c7a4ede7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library\n"
      ],
      "metadata": {
        "id": "_VX27if0zXnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from langchain.schema import Document\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "id": "Mfl-IHhKzZqU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune Model"
      ],
      "metadata": {
        "id": "wULW3rhOzbhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    if not json_data:\n",
        "        raise ValueError(\"JSON data is empty\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(json_data)} courses\")\n",
        "    return json_data"
      ],
      "metadata": {
        "id": "FpdSsI5dzdUB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(json_data, tokenizer):\n",
        "    processed_data = []\n",
        "\n",
        "    for item in json_data:\n",
        "        # Construct the input text\n",
        "        content = [\n",
        "            f\"Title: {item.get('title', 'N/A')}\",\n",
        "            f\"Units: {item.get('units', 'N/A')}\",\n",
        "            f\"Description: {item.get('description', 'N/A')}\",\n",
        "            f\"Grading: {item.get('grading', 'N/A')}\",\n",
        "            f\"Class Structure: {item.get('class_structure', 'Class structure not found')}\"\n",
        "        ]\n",
        "\n",
        "        # Add prerequisites, corequisites, and notes\n",
        "        if item.get('prerequisite(s)'):\n",
        "            content.append(\"Prerequisite(s): \" + \", \".join(item['prerequisite(s)']))\n",
        "\n",
        "        if item.get('corequisite(s)'):\n",
        "            content.append(\"Corequisite(s): \" + \", \".join(item['corequisite(s)']))\n",
        "\n",
        "        if item.get('pre/corequisite(s)'):\n",
        "            content.append(\"Pre/Corequisite(s): \" + \", \".join(item['pre/corequisite(s)']))\n",
        "\n",
        "        if item.get('notes'):\n",
        "            content.append(\"Note(s): \" + \", \".join(item['notes']))\n",
        "\n",
        "        # Handle core courses (for majors)\n",
        "        if 'core_courses' in item:\n",
        "            content.append(\"\\nCore Courses:\")\n",
        "            for course in item.get('core_courses', []):\n",
        "                content.append(f\"- {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "\n",
        "        # Handle specialization tracks (for majors)\n",
        "        if 'specialization_tracks' in item:\n",
        "            content.append(\"\\nSpecialization Tracks:\")\n",
        "            for specialization, details in item['specialization_tracks'].items():\n",
        "                content.append(f\"\\n- {specialization}:\")\n",
        "                if isinstance(details, list):  # MSAI-style specialization (direct list of courses)\n",
        "                    for course in details:\n",
        "                        content.append(f\"  - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "                elif isinstance(details, dict):  # MSSE-style specialization (nested dictionary)\n",
        "                    if 'overview' in details:\n",
        "                        content.append(f\"  Overview: {details['overview']}\")\n",
        "                    if 'required_core_courses' in details:\n",
        "                        content.append(\"\\n  Required Core Courses:\")\n",
        "                        for course in details['required_core_courses']:\n",
        "                            content.append(f\"    - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "                    if 'specialization_choice_courses' in details:\n",
        "                        content.append(\"\\n  Specialization Choice Courses:\")\n",
        "                        for course in details['specialization_choice_courses']:\n",
        "                            content.append(f\"    - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "\n",
        "        # Handle elective courses (for majors)\n",
        "        if 'elective_courses' in item:\n",
        "            content.append(\"\\nElective Courses:\")\n",
        "            if 'overview' in item['elective_courses']:\n",
        "                content.append(f\"  Overview: {item['elective_courses']['overview']}\")\n",
        "                if 'restricted_courses' in item['elective_courses']:\n",
        "                    content.append(\"\\n  Restricted Courses (cannot be taken as electives):\")\n",
        "                    for course in item['elective_courses']['restricted_courses']:\n",
        "                        if isinstance(course, dict):\n",
        "                            content.append(f\"    - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "                        elif isinstance(course, str):\n",
        "                            content.append(f\"    - {course}\")\n",
        "            else:\n",
        "                for area, courses in item['elective_courses'].items():\n",
        "                    content.append(f\"\\n- {area}:\")\n",
        "                    for course in courses:\n",
        "                        if isinstance(course, dict):\n",
        "                            content.append(f\"  - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "                        elif isinstance(course, str):\n",
        "                            content.append(f\"  - {course}\")\n",
        "\n",
        "        # Handle graduate writing requirement (for majors)\n",
        "        if 'graduate_writing_requirement' in item:\n",
        "            content.append(\"\\nGraduate Writing Requirement:\")\n",
        "            gww = item['graduate_writing_requirement']\n",
        "            if 'courses' in gww:\n",
        "                for course in gww['courses']:\n",
        "                    content.append(f\"  - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "                    if 'description' in course:\n",
        "                        content.append(f\"    Description: {course['description']}\")\n",
        "            elif 'course' in gww:\n",
        "                content.append(f\"  - {gww['course']}: {gww['title']} ({gww['units']} units)\")\n",
        "\n",
        "        # Handle culminating experience (for majors)\n",
        "        if 'culminating_experience' in item:\n",
        "            content.append(\"\\nCulminating Experience Options:\")\n",
        "            for option, courses in item['culminating_experience'].items():\n",
        "                content.append(f\"\\n- {option}:\")\n",
        "                for course in courses:\n",
        "                    if isinstance(course, dict):\n",
        "                        content.append(f\"  - {course['course']}: {course['title']} ({course['units']} units)\")\n",
        "\n",
        "        input_text = \"\\n\".join(content)\n",
        "\n",
        "        # Define the target text (e.g., a summary of the program or course)\n",
        "        if 'core_courses' in item:  # Major\n",
        "            target_text = (\n",
        "                f\"The {item.get('title', 'N/A')} program requires {item.get('units', 'N/A')} units, \"\n",
        "                f\"including core courses, specialization tracks, elective courses, a graduate writing requirement, \"\n",
        "                f\"and a culminating experience.\"\n",
        "            )\n",
        "        else:  # Course\n",
        "            target_text = (\n",
        "                f\"{item.get('title', 'N/A')} is a {item.get('units', 'N/A')} course. \"\n",
        "                f\"Description: {item.get('description', 'N/A')}\"\n",
        "            )\n",
        "\n",
        "        # Tokenize the input and target text\n",
        "        inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        targets = tokenizer(target_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        # Add to the processed dataset\n",
        "        processed_data.append({\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze().tolist(),  # Convert tensor to list\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze().tolist(),  # Convert tensor to list\n",
        "            \"labels\": targets[\"input_ids\"].squeeze().tolist()  # Convert tensor to list\n",
        "        })\n",
        "\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "ducbqYhx4x44"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for Flan-T5-Base\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Load the model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "AdalvO0QCr5S"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the dataset\n",
        "json_data = load_json_data(\"SJSU_courses_majors_dataset.json\")\n",
        "processed_data = process_data(json_data, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC6l0-vF0Hwe",
        "outputId": "7aa64dc0-ea42-455d-d4de-6832017ca4a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 5350 courses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert processed data to a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    \"input_ids\": [item[\"input_ids\"] for item in processed_data],\n",
        "    \"attention_mask\": [item[\"attention_mask\"] for item in processed_data],\n",
        "    \"labels\": [item[\"labels\"] for item in processed_data]\n",
        "})\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YisZ3pf5wLZ",
        "outputId": "1220c48f-2a79-40ca-d0dc-73a80bed0509"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 5350\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "val_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {
        "id": "d0eu2lNL1wDw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./flan-t5-finetuned\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,  # Reduce batch size\n",
        "    per_device_eval_batch_size=2,   # Reduce batch size\n",
        "    gradient_accumulation_steps=1,  # Gradient accumulation\n",
        "    fp16=False,  # Enable mixed precision\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    run_name=\"flan-t5-base-finetuned-run\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "Ye9TKrWo120-",
        "outputId": "63966418-6f1d-46fc-8990-64d3e93334c9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2408' max='2408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2408/2408 15:15, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2408, training_loss=0.0, metrics={'train_runtime': 916.468, 'train_samples_per_second': 5.254, 'train_steps_per_second': 2.627, 'total_flos': 3297106652037120.0, 'train_loss': 0.0, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./flan-t5-finetuned\")\n",
        "tokenizer.save_pretrained(\"./flan-t5-finetuned\")"
      ],
      "metadata": {
        "id": "1iJ3MUWv2Tea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./flan-t5-finetuned\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./flan-t5-finetuned\")\n",
        "\n",
        "# Example input\n",
        "input_text = \"Title: KIN 1 - Adapted Physical Activities\\nDescription: Structured individualized physical activities to enhance physical/motor fitness and develop an active, health-oriented lifestyle for students unable to participate in the general activity program.\"\n",
        "\n",
        "# Tokenize input\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids)\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Output:\", decoded_output)"
      ],
      "metadata": {
        "id": "RROitgmu2U5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}